#!/usr/bin/python

"""Read segmentation overlap measures of individual pairwise registrations
from CSV files generated by mirtk evaluate-overlap and compute the average
overlap for each class.
"""

import sys
import csv
import numpy as np
import argparse


def abbreviate(measure):
    """Get abbreviation of measure."""
    lstr = measure.lower()
    if lstr in ('sensitivity', 'truepositiverate', 'true positive rate'):
        return 'TPR'
    if lstr in ('specificity', 'truenegativerate', 'true negative rate'):
        return 'TNR'
    if lstr in ('positivepredictivevalue', 'positive predictive value', 'precision'):
        return 'PPV'
    if lstr == 'negativepredictivevalue':
        return 'NPV'
    if lstr == 'falsepositiverate':
        return 'FPR'
    if lstr == 'falsediscoveryrate':
        return 'FDR'
    if lstr == 'accuracy':
        return 'ACC'
    if lstr in ('informedness', 'bookmaker informedness'):
        return 'BM'
    if lstr in ('matthewscorrelation', 'matthewscorrelationcoefficient',
                'matthews correlation', 'matthews correlation coefficient'):
        return 'MCC'
    return measure


def evaluate_overlap(tp, fp, fn, tn, measure='Dice'):
    """Evaluate overlap measure given entries of confusion matrix."""
    measure = abbreviate(measure).lower()
    if measure == 'tpr':
        return float(tp) / float(tp + fn)
    if measure == 'tnr':
        return float(tn) / float(fp + tn)
    if measure == 'ppv':
        return float(tp) / float(tp + fp)
    if measure == 'npv':
        return float(tn) / float(tn + fn)
    if measure == 'fpr':
        return float(fp) / float(fp + tn)
    if measure == 'fdr':
        return float(fp) / float(fp + tp)
    if measure == 'fnr':
        return float(fn) / float(tp + fn)
    if measure == 'acc':
        return float(tp + tn) / float(tp + fn + fp + tn)
    if measure == 'bm':
        return float(tp) / float(tp + fn) + float(tn) / float(fp + tn) - 1.
    if measure == 'mcc':
        denom = sqrt(float(tp + fp) * float(tp + fn) * float(tn + fp) * float(tn + fn))
        return (float(tp) * float(tn) - float(fp) * float(fn)) / denom
    if measure.lower() == 'markedness':
        return float(tp) / float(tp + fp) + float(tn) / float(tn + fn) - 1.
    if measure.lower() in ('f1score', 'f1-score', 'f-score', 'fmeasure', 'f-measure', 'dice'):
        return float(2 * tp) / float((fp + tp) + (tp + fn))
    if measure == 'jaccard':
        return float(tp) / float(fp + tp + fn)
    else:
        raise Exception("Unknown overlap measure: {}".format(measure))


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument('tables', nargs='+',
                        help="List of CSV files.")
    parser.add_argument('--micro', action='store_true',
                        help="Compute micro-average of each measure instead of a macro-average")
    args = parser.parse_args()
    header = []
    labels = []
    with open(args.tables[0], 'rb') as f:
        reader = csv.reader(f)
        header = reader.next()
        for row in reader:
            if len(row) != len(header):
                raise Exception("Rows of CSV tables must have equal number of columns")
            labels.append(int(row[0]))
    tp_col = -1
    fp_col = -1
    fn_col = -1
    tn_col = -1
    for c in range(1, len(header)):
        lstr = header[c].lower()
        if lstr == 'tp':
            tp_col = c
        elif lstr == 'fp':
            fp_col = c
        elif lstr == 'fn':
            fn_col = c
        elif lstr == 'tn':
            tn_col = c
    if args.micro and (tp_col == -1 or fp_col == -1 or fn_col == -1 or tn_col == -1):
        raise Exception("Missing one or more of TP,FP,FN,TN columns needed for micro-averaging")
    sums = np.zeros((len(labels), len(header) - 1), dtype=np.float)
    for csv_name in args.tables:
        sums += np.genfromtxt(csv_name,
                              delimiter=',',
                              skip_header=1,
                              usecols=range(1, len(header)),
                              dtype=np.float)
    num = len(args.tables)
    out = sys.stdout
    try:
        if args.micro:
            cols = []
            for c in range(1, len(header)):
                if c not in (tp_col, fp_col, fn_col, tn_col):
                    cols.append(c)
            out.write(header[0])
            out.write(',')
            out.write(','.join([header[c] for c in cols]))
            out.write('\n')
            for l in range(0, len(labels)):
                out.write(str(labels[l]))
                for c in cols:
                    avg = evaluate_overlap(sums[l, tp_col - 1],
                                           sums[l, fp_col - 1],
                                           sums[l, fn_col - 1],
                                           sums[l, tn_col - 1],
                                           measure=header[c])
                    out.write(',')
                    out.write('{:.5f}'.format(avg))
                out.write('\n')
        else:
            out.write(','.join(header))
            out.write('\n')
            for l in range(0, len(labels)):
                out.write(str(labels[l]))
                for c in range(0, len(header) - 1):
                    avg = sums[l, c] / num
                    out.write(',')
                    out.write('{:.5f}'.format(avg))
                out.write('\n')
    finally:
        if out != sys.stdout:
            out.close()
